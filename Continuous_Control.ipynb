{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTINUOUS CONTROL - REACHER\n",
    "_Project for Udacity Deep Reinforcement Learning Nanodegree_\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview\n",
    "\n",
    "## Agent overview\n",
    "This code has the flexibility to run either __DDPG (Deep Deterministic Policy Gradient)__ or a __D4PG (Distributed Distributional Deep Deterministic Policy Gradient)__ agents in either the single arm or multi arm environments.\n",
    "\n",
    "### Deep Deterministic Policy Gradient Agent\n",
    "\n",
    "The DDPG uses an __Actor Critic__ architecture to estimate the best action for the current observation of the state. The __Actor__ is a policy based agent the uses a neural network to directly estimate the best policy for the agent while the __Critic__ is a value based agent that determines the value of the action in the given state represented by a _Q value_ given the current observation of the state and the output generated by the Actor.\n",
    "\n",
    "#### DDPG Learning Process\n",
    "The DDPG agent is an off policy agent that utilizes a __Replay Buffer__ to increase its sample efficiency by storing experiences (SARS' tuples) in the replay buffer and sampling batches of experiences to learn from. This allows experiences to be utilized several times to prior to being discarded hence allows an agent to learn the aspects of the environment with fewer observations. \n",
    "\n",
    "The DDPG agent consists of both __Active and Target__ networks, this is drawn from the concept of __Fixed Q Targets__ presented in the __DQN (Deep Q Network)__ to stabilizes learning by reducing the speed of change of the targets networks that the active network is working toward. This is further evolved in the concept of the DDPG by having target networks for both the Actor and the Critic and then utilizing them to train the Active Critic network.\n",
    "\n",
    "The Active Critic learns by calculating the __Mean Squared Error Loss__ between its value estimate for the action proposed by the actor for the current observation of the state and the sum of the current reward and the value estimate by the Target Critic for the action proposed by the Target Actor for the observation of the next state. Once the MSE Loss is calculated, the loss back propogated through the Active Critic network to train it using __Gradient Descent__.\n",
    "\n",
    "The gradients for the the Acitve Critic is clipped to prevent large updates that would destabilize the neural network\n",
    "\n",
    "The Active Actor is then updated with the negative of the value predicted by the Active Critic performing back propogation to train it using __Gradient Ascent__ in order to maximize the score as the score is a representation of value.\n",
    "\n",
    "\n",
    "### Distributed Distributional Deep Deterministic Policy Gradient Agent\n",
    "The D4PG is an evolution of the DDPG architecture implemented by __Google Deepmind__ recently. The D4PG paper introduces several key concepts to improve performance over the traditional DDPG architecture. These include:\n",
    "\n",
    "- N-Step Rollouts\n",
    "- Prioritized Experience Replay (Not Implemented)\n",
    "- K-Actor Distributed Training \n",
    "- Distributional Value Estimation\n",
    "\n",
    "_For simplicity, observation of state will be referred to as state._\n",
    "\n",
    "#### N-Step Rollouts\n",
    "N Step Rollouts are not a new concept but has been incorporated to include a better understanding of value of the state n steps from the current state. N steps is the middle ground between __Monte Carlo Learning__, where the agent learns at the end of every trajectory, and __Temporal Difference Learning__, where the agent learns at every timestep. By using the rollout length of the N steps, we can control how far ahead the agent looks. In this implementation, the __N Step Replay Buffer__ has the capacity to switch in between the standard single experience mode and N step mode. When in N step mode, the agent keeps track of experiences of a certain rollout length, traditionally 5 is used. Once the rollout length is reached, the agent simply converts the set of experiences into a single experience by changing the next state to the state at the N'th step and changing the reward to contain the initial reward and the discounted rewards till the N'th state\n",
    "\n",
    "#### K-Actor Distributed Training\n",
    "K-Actors are used to gather experience and fill the replay buffer, allowing rapid gathering of experience thereby enhancing learning quicker with a diverse set of experiences. \n",
    "\n",
    "#### Prioritized Replay\n",
    "This was not implemented as the agent was easily able to beat the environment. This and the implementation of __Hindsight Experience Replay__ will be really interesting to evaluate\n",
    "\n",
    "#### Distributional Value Estimation\n",
    "This allows the Critic to estimate the value more accurately by using a probability distribution to estimate the probability of a value. This allows the agent to understand the there is X% chance to result in a value but there also Y% chance to result in this value. This gives the Critic a wider perception of the possiblities thereby making the Critic perform better. Since the loss that is used to update the Actor is the negative of the value estimated by the Critic, improving the Critic will directly improve the Actor resulting in better performance of the agent.\n",
    "\n",
    "\n",
    "### Implemented Tweaks\n",
    "\n",
    "#### Reward Hacking\n",
    "I implemented reward hacking to adjust the rewards to attain better performance by creating a reward hypothesis which I personally felt was better suited for the environment. These hacked rewards are not used in the calculation of the scores, only in the creating of experience tuples hence it only affects the learning. \n",
    "\n",
    "Currently the agent receives a reward 0.1 for being in the correct position and 0 for being in the wrong position. I simply modified the rewards with somewhat of a wrapper to give a negative reward of -0.0001. This seems to further acclerated training as in one implementation, the agent was able to beat the environment in simply 16 episodes. This combined with the N step rollouts seems to have positive affects in the speed of convergence as the agent now experiences discounted penalties in the process of value estimation thereby instead of simply seeing a 0, it now sees a small negative number there which varies depending on how long the robotic arm was away from the optimal state. \n",
    "\n",
    "#### Batch Normalization\n",
    "Batch Normalization allows the normalization of the at each stage as it moves from the hidden layers output to the __Activation Function__ thereby levelizing and regularizing the data thereby leading to improved and more stable learning. However, this initially wrecked learning when implemented across all layers with __ReLU and Tanh Activations__. By removing the Batch Normalization layers prior to the ReLU activations and keeping the Batch Normalization prior to the Tanh activations resulted in much more stable learning even outperforming implementations with and Batch Normalization layers. I believe this is because Tanh simply outputs majority of values in between a small range unlike ReLU and is aided much more so with regularized inputs.\n",
    "\n",
    "#### Delayed Hard Target Network Updates\n",
    "Instead of using the traditional soft updates where the Target networks are updated with the parameters from the Active networks by scaling the parameters down with a factor of __TAU__. The Target networks were updated every x timesteps with a hard update from the Active networks. This seemed to stabilize learning further from my experience\n",
    "\n",
    "#### Pre - Training \n",
    "When the agent is started the agent generates experiences with the use of random actions without forward prograting through the network. This fills up the replay buffer quickly resulting in gathering experiences performed with a much broader exploration outlook. This seems to help the agent get through the essential first steps of learning by learning from a diverse set of experiences before simply using __Gaussian Noise__ for the actions creating somewhat of less of a diverse variety of experiences later on but closer to the specific direction in which the agent is heading.\n",
    "\n",
    "\n",
    "### Failed Tweaks\n",
    "\n",
    "#### Scheduled Learning Rates\n",
    "Scheduling the learning rates to reduce on performance plateaus were tested. This to cause problems when the agent hits local optimums as the agent drastically reduces its learning the longer its stuck in a local optima thereby hinders itself to moving out of the local optima by having its capacity to learn being pulled out right under it.\n",
    "\n",
    "_This could possibly prove to work if the patience factor in between the updates is increased thereby the agents learning rate is decreased less frequently_\n",
    "\n",
    "#### Cyclical Learning Rates\n",
    "Updating the learning rates periodically by multiplying the base learning to the oscillations of a __Cosine Function__ resulting in learning cycling between a value range. This seems to cause subtantial instability in the network and the agent sometimes gets thrown off a cliff once it has converged as the learning rate gets too high.\n",
    "\n",
    "_This could possibly prove to work if the base learning rate is kept lower preventing the agent from being thrown off a cliff_\n",
    "\n",
    "#### Parameter Noise\n",
    "This was suggested in OpenAIs paper in encourage exploration and that it performs better than both Gaussian and OU Noise allowing for learning in some sense similar to an __Evolutionary Alogrithm__. I tried adding little bits of noise to the network weights / parameters several times but this seemed to wreck any learning even.\n",
    "\n",
    "_This could possibly prove to work if extremely tiny bits of noise is added to the parameters. I find it questionable as it causes substantial instability in the environment_\n",
    "\n",
    "#### Delayed Intensive Updates\n",
    "This ideology was obtained from the Udacity benchmark implementation. I personally found this to result in greater instability resulting in slower convergence. For testing, I updated the agents every 20 timesteps 10 times in a row. \n",
    "\n",
    "_Honeslty speaking, I don't think there is much point in this tweak_\n",
    "\n",
    "\n",
    "### The Results \n",
    "\n",
    "The scores were tracked using Tensorboard along with the losses of the Actor and the Critic. The agents were trained from scratch several times with them converges every single time. Some times slower than others, I would further look into getting optimizations to reduce the variation in time of convergence but this could simply be due to random exploration during the first stages of pretraining and random noise generated by the Gaussian Noise\n",
    "\n",
    "#### Tracked by Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Title](images/Results.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameter Selection\n",
    "\n",
    "__ACTOR LEARNING RATE / CRITIC LEARNING RATE__ - Tests were run with several learning rates in between 0.0005 to 0.0001 for the Actor and learning rates in between 0.001 to 0.0005 for the Critic. The D4PG seems to be quite a robust network being able to eventually converge either which way\n",
    "\n",
    "__EPSILON__ - This is the control used to tweak the Gaussian Noise that is added to the action thereby it is the variable that we use to control exploration of the environment. This was set to 0.3 and not annealed throughout the training\n",
    "\n",
    "__PRETRAIN__ - The number of random experiences that are required to be generated and filled into the replay buffer before proceeding with training the agent using Gradient Descent\n",
    "\n",
    "__UPDATE_TARGETS_EVERY__ - This is the control used to determine how frequently the parameters from the Active networks are used to update the Target networks. This was set to 350\n",
    "\n",
    "__ROLLOUT LENGTH__ - This is used to determine the rollout length used by the N step replay buffer to decide how far ahead the agent should look into\n",
    "\n",
    "__ATOMS__ - This controls the granularity with which the probability distribution is estimated from the Q network. The more atoms there are, the greater the granularity of the distribution. \n",
    "\n",
    "__VMIN / VMAN__ - The bounds of the values predicted by the agent. The atoms are distributed between these two values which in turn are multiplied by the probabilities from the Critic network to arrive at the value distribution. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initialize the Environment \n",
    "Run the below section once to initialize the environment.. Don't Repeat!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Number of actions: 4\n",
      "States have length: 33\n",
      "States initialized: 20\n"
     ]
    }
   ],
   "source": [
    "from utilities import Seeds, initialize_env, get_device\n",
    "\n",
    "MULTI = True\n",
    "device = get_device()                           # gets gpu if available\n",
    "\n",
    "environment_params = {\n",
    "    'multiple_agents': MULTI,                   # runs 20 or 1 arm environment\n",
    "    'no_graphics': False,                       # runs no graphics windows version\n",
    "    'train_mode': True,                         # runs in train mode\n",
    "    'offline': True,                            # toggle on for udacity jupyter notebook\n",
    "    'agent_count': 20 if MULTI else 1,  \n",
    "    'device': device\n",
    "}\n",
    "\n",
    "env, env_info, states, state_size, action_size, brain_name, num_agents = initialize_env(environment_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configure the Agent\n",
    "Configures the respective 'knobs' on the agent to beat the environment.. The best configurations from my experiments have already been set.\n",
    "\n",
    "To load the complete version of the agent, please run as is. If you want test the agent from scratch, simply change the name in agent params and re run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from agent import D4PGAgent\n",
    "from train import train\n",
    "from memory import NStepReplayBuffer\n",
    "from noise import OUNoise, GaussianExploration\n",
    "\n",
    "seedGenerator = Seeds('seeds')\n",
    "seedGenerator.next()\n",
    "\n",
    "experience_params = {\n",
    "    'seed': seedGenerator,                      # seed for the experience replay buffer\n",
    "    'buffer_size': 300000,                      # size of the replay buffer\n",
    "    'batch_size': 128,                          # batch size sampled from the replay buffer\n",
    "    'rollout_length': 5,                        # n step rollout length    \n",
    "    'agent_count': 20 if MULTI else 1,  \n",
    "    'gamma': 0.99,\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "experienceReplay = NStepReplayBuffer(experience_params)\n",
    "\n",
    "noise_params = {\n",
    "    'ou_noise_params': {                        # parameters for the Ornstein Uhlenbeck process\n",
    "        'mu': 0.,                               # mean\n",
    "        'theta': 0.15,                          # theta value for the ornstein-uhlenbeck process\n",
    "        'sigma': 0.2,                           # variance\n",
    "        'seed': seedGenerator,                  # seed\n",
    "        'action_size': action_size  \n",
    "    },  \n",
    "    'ge_noise_params': {                        # parameters for the Gaussian Exploration process                   \n",
    "        'max_epsilon': 0.3,                     \n",
    "        'min_epsilon': 0.005,   \n",
    "        'decay_epsilon': True,      \n",
    "        'patience_episodes': 2,                 # episodes since the last best reward  \n",
    "        'decay_rate': 0.95                   \n",
    "    }\n",
    "}\n",
    "\n",
    "noise = GaussianExploration(noise_params['ge_noise_params'])\n",
    "\n",
    "params = {\n",
    "    'episodes': 2000,                           # number of episodes\n",
    "    'maxlen': 100,                              # sliding window size of recent scores\n",
    "    'brain_name': brain_name,                   # the brain name of the unity environment\n",
    "    'achievement': 30.,                         # score at which the environment is considered beaten\n",
    "    'achievement_length': 100,                  # how long the agent needs to get a score above the achievement to solve the environment\n",
    "    'environment': env,             \n",
    "    'pretrain': True,                           # whether pretraining with random actions should be done\n",
    "    'pretrain_length': 20000,                   # minimum experience required in replay buffer to start training \n",
    "    'random_fill': False,                       # basically repeat pretrain at specific times to encourage further exploration\n",
    "    'random_fill_every': 10000,             \n",
    "    'hack_rewards': True,                       # shapes 0 rewards into small negative rewards\n",
    "    'negative_reward': -0.0001,\n",
    "    'log_dir': 'runs/',\n",
    "    'load_agent': True,\n",
    "    'agent_params': {\n",
    "        'name': 'D4PG Jupyter Notebook',\n",
    "        'd4pg': True,\n",
    "        'experience_replay': experienceReplay,\n",
    "        'device': device,\n",
    "        'seed': seedGenerator,\n",
    "        'num_agents': num_agents,               # number of agents in the environment\n",
    "        'gamma': 0.99,                          # discount factor\n",
    "        'tau': 0.0001,                          # mixing rate soft-update of target parameters\n",
    "        'update_target_every': 350,             # update the target network every n-th step\n",
    "        'update_every': 1,                      # update the active network every n-th step\n",
    "        'actor_update_every_multiplier': 1,     # update actor every x timestep multiples of the crtic, critic needs time to adapt to new actor\n",
    "        'update_intensity': 1,                  # learns from the same experiences several times\n",
    "        'update_target_type': 'hard',           # should the update be soft at every time step or hard at every x timesteps\n",
    "        'add_noise': True,                      # add noise using 'noise_params'\n",
    "        'schedule_lr': False,                   # schedule learning rates \n",
    "        'lr_steps': 30,                         # step iterations to cycle lr using cosine\n",
    "        'lr_reset_every': 5000,                 # steps learning rate   \n",
    "        'lr_reduction_factor': 0.9,             # reduce lr on plateau reduction factor\n",
    "        'lr_patience_factor': 10,               # reduce lr after x (timesteps/episodes) not changing tracked item\n",
    "        'actor_params': {                       # actor parameters\n",
    "            'lr': 0.0001,                       # learning rate\n",
    "            'state_size': state_size,           # size of the state space\n",
    "            'action_size': action_size,         # size of the action space\n",
    "            'seed': seedGenerator,              # seed of the network architecture\n",
    "        },\n",
    "        'critic_params': {                      # critic parameters\n",
    "            'lr': 0.0005,                       # learning rate\n",
    "            'weight_decay': 3e-10,              # weight decay\n",
    "            'state_size': state_size,           # size of the state space\n",
    "            'action_size': action_size,         # size of the action space\n",
    "            'seed': seedGenerator,              # seed of the network architecture\n",
    "            'action_layer': True,\n",
    "            'num_atoms': 75,\n",
    "            'v_min': 0.0, \n",
    "            'v_max': 0.5\n",
    "        },\n",
    "        'noise': noise\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agent\n",
    "Get the agent to learn to beat the environment and output the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################ ACTOR ################\n",
      "\n",
      "Actor(\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=33, out_features=400, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc2): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=300, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc3): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=4, bias=True)\n",
      "    (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Tanh()\n",
      "  )\n",
      ")\n",
      "\n",
      "################ CRITIC ################\n",
      "\n",
      "D4PGCritic(\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=37, out_features=400, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc2): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=300, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc3): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=75, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n",
      "\n",
      "Cannot find D4PG Jupyter Notebook checkpoint... Proceeding to create fresh neural network\n",
      "\n",
      "Episode 1\tAverage Score: 0.10 \t Min: 0.00 \t Max: 0.54 \t Time: 4.98\n",
      "Episode 2\tAverage Score: 0.28 \t Min: 0.00 \t Max: 1.14 \t Time: 98.39\n",
      "Episode 3\tAverage Score: 1.04 \t Min: 0.04 \t Max: 2.37 \t Time: 95.02\n",
      "Episode 4\tAverage Score: 2.30 \t Min: 0.93 \t Max: 3.86 \t Time: 96.68\n",
      "Episode 5\tAverage Score: 3.40 \t Min: 1.97 \t Max: 5.35 \t Time: 99.93\n",
      "Episode 6\tAverage Score: 5.48 \t Min: 2.07 \t Max: 7.86 \t Time: 96.20\n",
      "Episode 7\tAverage Score: 7.08 \t Min: 4.48 \t Max: 9.06 \t Time: 95.78\n",
      "Episode 8\tAverage Score: 9.72 \t Min: 6.91 \t Max: 12.23 \t Time: 97.36\n",
      "Episode 9\tAverage Score: 9.73 \t Min: 6.86 \t Max: 12.45 \t Time: 95.82\n",
      "Episode 10\tAverage Score: 10.83 \t Min: 8.45 \t Max: 13.61 \t Time: 95.24\n",
      "Episode 11\tAverage Score: 9.52 \t Min: 6.45 \t Max: 12.47 \t Time: 95.54\n",
      "Episode 12\tAverage Score: 11.33 \t Min: 7.62 \t Max: 14.40 \t Time: 95.93\n",
      "Episode 13\tAverage Score: 10.06 \t Min: 7.70 \t Max: 12.03 \t Time: 96.07\n",
      "Episode 14\tAverage Score: 9.45 \t Min: 7.09 \t Max: 11.79 \t Time: 95.83\n",
      "Episode 15\tAverage Score: 9.71 \t Min: 6.78 \t Max: 13.20 \t Time: 95.57\n",
      "Episode 16\tAverage Score: 8.79 \t Min: 6.90 \t Max: 12.59 \t Time: 95.71\n",
      "Episode 17\tAverage Score: 9.12 \t Min: 6.77 \t Max: 11.06 \t Time: 96.52\n",
      "Episode 18\tAverage Score: 7.98 \t Min: 6.22 \t Max: 9.23 \t Time: 95.81\n",
      "Episode 19\tAverage Score: 9.22 \t Min: 6.52 \t Max: 11.65 \t Time: 96.92\n",
      "Episode 20\tAverage Score: 8.76 \t Min: 6.14 \t Max: 11.22 \t Time: 95.51\n",
      "Episode 21\tAverage Score: 8.28 \t Min: 5.95 \t Max: 11.09 \t Time: 96.90\n",
      "Episode 22\tAverage Score: 9.21 \t Min: 6.92 \t Max: 12.09 \t Time: 96.80\n",
      "Episode 23\tAverage Score: 5.96 \t Min: 4.28 \t Max: 7.07 \t Time: 94.80\n",
      "Episode 24\tAverage Score: 5.86 \t Min: 4.81 \t Max: 7.13 \t Time: 97.15\n",
      "Episode 25\tAverage Score: 6.18 \t Min: 4.21 \t Max: 7.60 \t Time: 95.32\n",
      "Episode 26\tAverage Score: 5.88 \t Min: 4.72 \t Max: 7.03 \t Time: 93.62\n",
      "Episode 27\tAverage Score: 6.18 \t Min: 5.15 \t Max: 7.38 \t Time: 93.25\n",
      "Episode 28\tAverage Score: 5.63 \t Min: 3.77 \t Max: 7.97 \t Time: 95.80\n",
      "Episode 29\tAverage Score: 5.34 \t Min: 4.22 \t Max: 7.57 \t Time: 95.73\n",
      "Episode 30\tAverage Score: 5.46 \t Min: 4.40 \t Max: 7.21 \t Time: 98.26\n",
      "Episode 31\tAverage Score: 6.96 \t Min: 5.70 \t Max: 8.67 \t Time: 100.56\n",
      "Episode 32\tAverage Score: 6.77 \t Min: 5.28 \t Max: 9.25 \t Time: 95.08\n",
      "Episode 33\tAverage Score: 5.87 \t Min: 4.63 \t Max: 6.93 \t Time: 94.58\n",
      "Episode 34\tAverage Score: 6.33 \t Min: 4.55 \t Max: 8.05 \t Time: 94.45\n",
      "Episode 35\tAverage Score: 5.47 \t Min: 3.77 \t Max: 6.86 \t Time: 94.19\n",
      "Episode 36\tAverage Score: 5.06 \t Min: 3.56 \t Max: 6.43 \t Time: 93.92\n",
      "Episode 37\tAverage Score: 5.74 \t Min: 4.34 \t Max: 8.12 \t Time: 94.64\n",
      "Episode 38\tAverage Score: 5.80 \t Min: 5.12 \t Max: 7.45 \t Time: 93.75\n",
      "Episode 39\tAverage Score: 5.79 \t Min: 4.75 \t Max: 7.79 \t Time: 93.57\n",
      "Episode 40\tAverage Score: 5.47 \t Min: 4.48 \t Max: 6.42 \t Time: 93.12\n",
      "Episode 41\tAverage Score: 5.42 \t Min: 4.03 \t Max: 6.57 \t Time: 93.05\n",
      "Episode 42\tAverage Score: 5.71 \t Min: 4.83 \t Max: 6.81 \t Time: 92.84\n",
      "Episode 43\tAverage Score: 5.47 \t Min: 4.18 \t Max: 6.38 \t Time: 92.63\n",
      "Episode 44\tAverage Score: 5.03 \t Min: 4.23 \t Max: 6.07 \t Time: 92.80\n",
      "Episode 45\tAverage Score: 5.12 \t Min: 4.19 \t Max: 5.79 \t Time: 92.21\n",
      "Episode 46\tAverage Score: 5.25 \t Min: 4.32 \t Max: 6.63 \t Time: 92.53\n",
      "Episode 47\tAverage Score: 5.78 \t Min: 4.91 \t Max: 6.71 \t Time: 93.12\n",
      "Episode 48\tAverage Score: 5.73 \t Min: 4.98 \t Max: 6.57 \t Time: 93.35\n",
      "Episode 49\tAverage Score: 6.21 \t Min: 5.29 \t Max: 7.25 \t Time: 92.14\n",
      "Episode 50\tAverage Score: 5.31 \t Min: 3.96 \t Max: 6.35 \t Time: 92.93\n",
      "Episode 51\tAverage Score: 4.39 \t Min: 3.41 \t Max: 6.57 \t Time: 93.28\n",
      "Episode 52\tAverage Score: 4.63 \t Min: 3.42 \t Max: 6.05 \t Time: 92.72\n",
      "Episode 53\tAverage Score: 4.64 \t Min: 3.73 \t Max: 5.91 \t Time: 92.40\n",
      "Episode 54\tAverage Score: 5.11 \t Min: 4.28 \t Max: 6.04 \t Time: 92.88\n",
      "Episode 55\tAverage Score: 4.64 \t Min: 3.92 \t Max: 5.45 \t Time: 92.51\n",
      "Episode 56\tAverage Score: 4.06 \t Min: 3.43 \t Max: 4.74 \t Time: 92.62\n",
      "Episode 57\tAverage Score: 4.91 \t Min: 4.15 \t Max: 5.45 \t Time: 92.41\n",
      "Episode 58\tAverage Score: 5.01 \t Min: 4.38 \t Max: 5.66 \t Time: 92.88\n",
      "Episode 59\tAverage Score: 5.23 \t Min: 4.42 \t Max: 6.50 \t Time: 92.95\n",
      "Episode 60\tAverage Score: 4.97 \t Min: 4.13 \t Max: 5.92 \t Time: 92.84\n",
      "Episode 61\tAverage Score: 5.14 \t Min: 4.30 \t Max: 6.02 \t Time: 92.97\n",
      "Episode 62\tAverage Score: 4.98 \t Min: 4.11 \t Max: 5.94 \t Time: 93.20\n",
      "Episode 63\tAverage Score: 5.29 \t Min: 4.66 \t Max: 6.01 \t Time: 92.83\n",
      "Episode 64\tAverage Score: 5.38 \t Min: 4.04 \t Max: 6.52 \t Time: 93.43\n",
      "Episode 65\tAverage Score: 5.65 \t Min: 4.98 \t Max: 6.34 \t Time: 92.89\n",
      "Episode 66\tAverage Score: 5.27 \t Min: 4.46 \t Max: 6.21 \t Time: 93.18\n",
      "Episode 67\tAverage Score: 5.15 \t Min: 4.40 \t Max: 5.98 \t Time: 93.56\n",
      "Episode 68\tAverage Score: 4.70 \t Min: 3.43 \t Max: 5.79 \t Time: 92.98\n",
      "Episode 69\tAverage Score: 5.51 \t Min: 4.95 \t Max: 6.12 \t Time: 93.23\n",
      "Episode 70\tAverage Score: 5.58 \t Min: 4.75 \t Max: 6.36 \t Time: 92.92\n",
      "Episode 71\tAverage Score: 5.52 \t Min: 4.45 \t Max: 6.50 \t Time: 92.71\n",
      "Episode 72\tAverage Score: 5.11 \t Min: 4.40 \t Max: 6.30 \t Time: 92.54\n",
      "Episode 73\tAverage Score: 5.43 \t Min: 5.04 \t Max: 5.78 \t Time: 92.49\n",
      "Episode 74\tAverage Score: 5.40 \t Min: 4.71 \t Max: 6.28 \t Time: 93.03\n",
      "Episode 75\tAverage Score: 5.10 \t Min: 4.47 \t Max: 6.02 \t Time: 92.78\n",
      "Episode 76\tAverage Score: 4.73 \t Min: 3.97 \t Max: 5.48 \t Time: 92.75\n",
      "Episode 77\tAverage Score: 4.59 \t Min: 3.88 \t Max: 5.36 \t Time: 93.25\n",
      "Episode 78\tAverage Score: 4.93 \t Min: 4.50 \t Max: 5.62 \t Time: 92.76\n",
      "Episode 79\tAverage Score: 5.05 \t Min: 4.19 \t Max: 6.00 \t Time: 92.84\n",
      "Episode 80\tAverage Score: 5.01 \t Min: 4.25 \t Max: 5.67 \t Time: 92.37\n",
      "Episode 81\tAverage Score: 4.80 \t Min: 3.93 \t Max: 5.57 \t Time: 92.69\n",
      "Episode 82\tAverage Score: 4.60 \t Min: 3.99 \t Max: 5.42 \t Time: 92.81\n",
      "Episode 83\tAverage Score: 4.95 \t Min: 4.34 \t Max: 5.59 \t Time: 93.19\n",
      "Episode 84\tAverage Score: 5.22 \t Min: 4.21 \t Max: 6.15 \t Time: 92.07\n",
      "Episode 85\tAverage Score: 4.93 \t Min: 4.12 \t Max: 5.82 \t Time: 92.21\n",
      "Episode 86\tAverage Score: 4.66 \t Min: 3.85 \t Max: 5.64 \t Time: 92.24\n",
      "Episode 87\tAverage Score: 4.81 \t Min: 4.02 \t Max: 5.51 \t Time: 92.62\n",
      "Episode 88\tAverage Score: 19.23 \t Min: 14.57 \t Max: 24.40 \t Time: 92.12\n",
      "Episode 89\tAverage Score: 18.02 \t Min: 11.88 \t Max: 26.31 \t Time: 92.53\n",
      "Episode 90\tAverage Score: 24.41 \t Min: 15.55 \t Max: 30.89 \t Time: 93.29\n",
      "Episode 91\tAverage Score: 26.09 \t Min: 18.55 \t Max: 35.32 \t Time: 93.01\n",
      "Episode 92\tAverage Score: 29.71 \t Min: 23.30 \t Max: 36.38 \t Time: 93.25\n",
      "Episode 93\tAverage Score: 28.41 \t Min: 23.47 \t Max: 36.50 \t Time: 93.46\n",
      "Episode 94\tAverage Score: 28.42 \t Min: 17.50 \t Max: 36.08 \t Time: 93.21\n",
      "Episode 95\tAverage Score: 31.31 \t Min: 26.18 \t Max: 35.78 \t Time: 93.50\n",
      "Episode 96\tAverage Score: 30.33 \t Min: 22.26 \t Max: 37.63 \t Time: 93.49\n",
      "Episode 97\tAverage Score: 31.25 \t Min: 26.55 \t Max: 38.05 \t Time: 92.91\n",
      "Episode 98\tAverage Score: 31.21 \t Min: 22.46 \t Max: 36.26 \t Time: 93.07\n",
      "Episode 99\tAverage Score: 31.58 \t Min: 26.98 \t Max: 34.91 \t Time: 93.24\n",
      "Episode 100\tAverage Score: 33.00 \t Min: 28.80 \t Max: 38.06 \t Time: 93.48\n",
      "Episode 101\tAverage Score: 33.08 \t Min: 26.65 \t Max: 37.11 \t Time: 93.04\n",
      "Episode 102\tAverage Score: 32.89 \t Min: 26.44 \t Max: 36.27 \t Time: 93.30\n",
      "Episode 103\tAverage Score: 33.42 \t Min: 30.36 \t Max: 37.79 \t Time: 92.99\n",
      "Episode 104\tAverage Score: 33.01 \t Min: 28.97 \t Max: 36.93 \t Time: 92.59\n",
      "Episode 105\tAverage Score: 35.41 \t Min: 32.75 \t Max: 37.81 \t Time: 92.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 106\tAverage Score: 33.68 \t Min: 25.85 \t Max: 37.51 \t Time: 92.94\n",
      "Episode 107\tAverage Score: 32.93 \t Min: 29.39 \t Max: 35.64 \t Time: 92.63\n",
      "Episode 108\tAverage Score: 33.62 \t Min: 27.37 \t Max: 37.63 \t Time: 93.02\n",
      "Episode 109\tAverage Score: 33.61 \t Min: 23.47 \t Max: 37.30 \t Time: 93.14\n",
      "Episode 110\tAverage Score: 32.81 \t Min: 26.05 \t Max: 37.05 \t Time: 92.98\n",
      "Episode 111\tAverage Score: 32.66 \t Min: 28.44 \t Max: 37.99 \t Time: 92.92\n",
      "Episode 112\tAverage Score: 32.70 \t Min: 26.41 \t Max: 37.73 \t Time: 93.22\n",
      "Episode 113\tAverage Score: 31.22 \t Min: 24.05 \t Max: 36.60 \t Time: 92.06\n",
      "Episode 114\tAverage Score: 31.51 \t Min: 25.64 \t Max: 37.53 \t Time: 92.34\n",
      "Episode 115\tAverage Score: 30.15 \t Min: 24.30 \t Max: 36.56 \t Time: 92.40\n",
      "Episode 116\tAverage Score: 32.08 \t Min: 27.15 \t Max: 37.37 \t Time: 92.65\n",
      "Episode 117\tAverage Score: 32.93 \t Min: 27.42 \t Max: 36.37 \t Time: 92.38\n",
      "Episode 118\tAverage Score: 31.24 \t Min: 23.84 \t Max: 34.92 \t Time: 92.78\n",
      "Episode 119\tAverage Score: 30.72 \t Min: 23.72 \t Max: 36.40 \t Time: 93.19\n",
      "Episode 120\tAverage Score: 31.42 \t Min: 25.53 \t Max: 34.98 \t Time: 92.66\n",
      "Episode 121\tAverage Score: 32.31 \t Min: 28.10 \t Max: 36.63 \t Time: 92.91\n",
      "Episode 122\tAverage Score: 31.96 \t Min: 25.86 \t Max: 37.00 \t Time: 93.50\n",
      "Episode 123\tAverage Score: 30.57 \t Min: 26.23 \t Max: 33.47 \t Time: 92.97\n",
      "Episode 124\tAverage Score: 31.40 \t Min: 28.79 \t Max: 33.56 \t Time: 92.87\n",
      "Episode 125\tAverage Score: 32.99 \t Min: 27.42 \t Max: 36.92 \t Time: 93.93\n",
      "Episode 126\tAverage Score: 31.41 \t Min: 27.78 \t Max: 34.52 \t Time: 92.75\n",
      "Episode 127\tAverage Score: 31.72 \t Min: 29.00 \t Max: 33.62 \t Time: 92.71\n",
      "Episode 128\tAverage Score: 32.53 \t Min: 30.32 \t Max: 34.81 \t Time: 93.69\n",
      "Episode 129\tAverage Score: 32.87 \t Min: 26.74 \t Max: 37.42 \t Time: 93.43\n",
      "Episode 130\tAverage Score: 33.28 \t Min: 30.48 \t Max: 35.35 \t Time: 93.17\n",
      "Episode 131\tAverage Score: 32.17 \t Min: 27.97 \t Max: 35.39 \t Time: 92.99\n",
      "Episode 132\tAverage Score: 29.22 \t Min: 26.93 \t Max: 32.62 \t Time: 93.12\n",
      "Episode 133\tAverage Score: 33.25 \t Min: 30.44 \t Max: 35.57 \t Time: 93.70\n",
      "Episode 134\tAverage Score: 30.54 \t Min: 25.52 \t Max: 35.38 \t Time: 93.04\n",
      "Episode 135\tAverage Score: 33.15 \t Min: 28.59 \t Max: 36.92 \t Time: 92.84\n",
      "Episode 136\tAverage Score: 32.41 \t Min: 29.14 \t Max: 34.80 \t Time: 92.74\n",
      "Episode 137\tAverage Score: 31.99 \t Min: 28.22 \t Max: 36.70 \t Time: 92.90\n",
      "Episode 138\tAverage Score: 31.85 \t Min: 27.46 \t Max: 35.16 \t Time: 93.41\n",
      "Episode 139\tAverage Score: 33.83 \t Min: 31.62 \t Max: 35.76 \t Time: 93.04\n",
      "Episode 140\tAverage Score: 32.34 \t Min: 27.26 \t Max: 35.40 \t Time: 93.35\n",
      "Episode 141\tAverage Score: 32.81 \t Min: 28.77 \t Max: 35.69 \t Time: 92.83\n",
      "Episode 142\tAverage Score: 32.89 \t Min: 31.36 \t Max: 34.30 \t Time: 92.69\n",
      "Episode 143\tAverage Score: 32.71 \t Min: 28.28 \t Max: 35.44 \t Time: 92.63\n",
      "Episode 144\tAverage Score: 33.99 \t Min: 30.72 \t Max: 37.50 \t Time: 92.83\n",
      "Episode 145\tAverage Score: 32.33 \t Min: 29.19 \t Max: 35.26 \t Time: 93.03\n",
      "Episode 146\tAverage Score: 32.22 \t Min: 27.26 \t Max: 36.10 \t Time: 94.00\n",
      "Episode 147\tAverage Score: 29.84 \t Min: 27.13 \t Max: 32.94 \t Time: 92.86\n",
      "Episode 148\tAverage Score: 30.47 \t Min: 27.19 \t Max: 32.94 \t Time: 95.28\n",
      "Episode 149\tAverage Score: 29.09 \t Min: 25.49 \t Max: 32.15 \t Time: 93.35\n",
      "Episode 150\tAverage Score: 31.64 \t Min: 26.99 \t Max: 35.71 \t Time: 94.76\n",
      "Episode 151\tAverage Score: 32.31 \t Min: 27.17 \t Max: 35.85 \t Time: 94.38\n",
      "Episode 152\tAverage Score: 30.82 \t Min: 27.43 \t Max: 33.30 \t Time: 94.69\n",
      "Episode 153\tAverage Score: 31.11 \t Min: 25.20 \t Max: 36.09 \t Time: 96.46\n",
      "Episode 154\tAverage Score: 30.84 \t Min: 27.09 \t Max: 35.38 \t Time: 95.02\n",
      "Episode 155\tAverage Score: 31.70 \t Min: 28.56 \t Max: 34.82 \t Time: 94.97\n",
      "Episode 156\tAverage Score: 32.14 \t Min: 24.68 \t Max: 36.15 \t Time: 95.21\n",
      "Episode 157\tAverage Score: 30.77 \t Min: 25.59 \t Max: 36.42 \t Time: 95.40\n",
      "Episode 158\tAverage Score: 32.42 \t Min: 27.04 \t Max: 37.01 \t Time: 95.20\n",
      "Episode 159\tAverage Score: 31.22 \t Min: 26.13 \t Max: 33.99 \t Time: 95.52\n",
      "Episode 160\tAverage Score: 31.40 \t Min: 26.66 \t Max: 36.20 \t Time: 94.26\n",
      "Episode 161\tAverage Score: 31.68 \t Min: 24.98 \t Max: 36.13 \t Time: 94.40\n",
      "Episode 162\tAverage Score: 30.73 \t Min: 23.99 \t Max: 36.94 \t Time: 94.31\n",
      "Episode 163\tAverage Score: 31.27 \t Min: 24.96 \t Max: 34.83 \t Time: 95.45\n",
      "Episode 164\tAverage Score: 31.46 \t Min: 27.88 \t Max: 34.32 \t Time: 95.35\n",
      "Episode 165\tAverage Score: 30.69 \t Min: 21.82 \t Max: 33.71 \t Time: 94.57\n",
      "Episode 166\tAverage Score: 28.88 \t Min: 25.90 \t Max: 32.55 \t Time: 95.29\n",
      "Episode 167\tAverage Score: 32.01 \t Min: 28.84 \t Max: 35.25 \t Time: 99.59\n",
      "Episode 168\tAverage Score: 32.06 \t Min: 24.45 \t Max: 36.47 \t Time: 96.62\n",
      "Episode 169\tAverage Score: 30.83 \t Min: 25.97 \t Max: 34.58 \t Time: 95.96\n",
      "Episode 170\tAverage Score: 32.61 \t Min: 28.71 \t Max: 36.30 \t Time: 95.32\n",
      "Episode 171\tAverage Score: 33.11 \t Min: 29.38 \t Max: 37.02 \t Time: 94.97\n",
      "Episode 172\tAverage Score: 32.51 \t Min: 27.99 \t Max: 35.14 \t Time: 94.85\n",
      "Episode 173\tAverage Score: 31.75 \t Min: 28.98 \t Max: 34.83 \t Time: 95.69\n",
      "Episode 174\tAverage Score: 31.03 \t Min: 26.50 \t Max: 34.95 \t Time: 96.00\n",
      "Episode 175\tAverage Score: 32.90 \t Min: 28.75 \t Max: 36.93 \t Time: 95.04\n",
      "Episode 176\tAverage Score: 31.54 \t Min: 25.83 \t Max: 34.30 \t Time: 96.62\n",
      "Episode 177\tAverage Score: 32.60 \t Min: 29.96 \t Max: 35.36 \t Time: 99.43\n",
      "Episode 178\tAverage Score: 31.84 \t Min: 27.19 \t Max: 34.38 \t Time: 97.90\n",
      "Episode 179\tAverage Score: 30.14 \t Min: 27.54 \t Max: 32.73 \t Time: 97.88\n",
      "Episode 180\tAverage Score: 30.72 \t Min: 26.35 \t Max: 33.41 \t Time: 97.61\n",
      "Episode 181\tAverage Score: 29.64 \t Min: 23.39 \t Max: 32.38 \t Time: 97.62\n",
      "Episode 182\tAverage Score: 30.47 \t Min: 27.47 \t Max: 33.05 \t Time: 98.07\n",
      "Episode 183\tAverage Score: 31.19 \t Min: 26.57 \t Max: 34.29 \t Time: 97.39\n",
      "Episode 184\tAverage Score: 30.55 \t Min: 27.88 \t Max: 34.40 \t Time: 97.93\n",
      "Episode 185\tAverage Score: 30.30 \t Min: 26.01 \t Max: 33.74 \t Time: 98.17\n",
      "Episode 186\tAverage Score: 29.94 \t Min: 26.03 \t Max: 34.09 \t Time: 98.43\n",
      "Episode 187\tAverage Score: 32.16 \t Min: 29.20 \t Max: 35.22 \t Time: 97.68\n",
      "Timestep 187920\tScore: 30.12\tmin: 26.22\tmax: 33.52"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-69ffd0cc2d08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0magents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mD4PGAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'agent_params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_processes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_agents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'episode'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'D4PG'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Dropbox\\Coding\\AI\\Udacity\\Deep Reinforcement Learning\\deep-reinforcement-learning\\p2_continuous-control\\train.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(agents, params, num_processes)\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[0madjusted_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0madjusted_rewards\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'negative_reward'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m             \u001b[0mactor_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madjusted_rewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpretrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpretrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mactor_loss\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcritic_loss\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Dropbox\\Coding\\AI\\Udacity\\Deep Reinforcement Learning\\deep-reinforcement-learning\\p2_continuous-control\\agent.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, states, actions, rewards, next_states, dones, pretrain)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpretrain\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Dropbox\\Coding\\AI\\Udacity\\Deep Reinforcement Learning\\deep-reinforcement-learning\\p2_continuous-control\\agent.py\u001b[0m in \u001b[0;36mlearn_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    286\u001b[0m                     \u001b[1;31m# Calculate the projected log probabilities from the target actor and critic networks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m                     \u001b[1;31m# Since back propogation is not required. Tensors are detach to increase speed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m                     \u001b[0mtarget_dist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m                     \u001b[1;31m# The critic loss is calculated using a weighted distribution instead of the mean to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Dropbox\\Coding\\AI\\Udacity\\Deep Reinforcement Learning\\deep-reinforcement-learning\\p2_continuous-control\\agent.py\u001b[0m in \u001b[0;36m_get_targets\u001b[1;34m(self, rewards, next_states)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[1;31m# Project the categorical distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m         \u001b[0mprojected_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mprojected_probs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Dropbox\\Coding\\AI\\Udacity\\Deep Reinforcement Learning\\deep-reinforcement-learning\\p2_continuous-control\\agent.py\u001b[0m in \u001b[0;36m_get_value_distribution\u001b[1;34m(self, rewards, probs)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m             \u001b[0mprojected_probs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_add_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower_bound\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_lower\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m             \u001b[0mprojected_probs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_add_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupper_bound\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_upper\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mprojected_probs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agents = D4PGAgent(params=params['agent_params']) \n",
    "\n",
    "scores = train(agents=agents, params=params, num_processes=num_agents)\n",
    "\n",
    "df = pd.DataFrame(data={'episode': np.arange(len(scores)), 'D4PG': scores})\n",
    "df.to_csv('results/D4PG.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Improvements\n",
    "\n",
    "#### Prioritized Experience Replay\n",
    "By prioritizing the experiences and learning from experiences with 'more to learn from' will most likely result to faster convergence. \n",
    "\n",
    "#### Annealing of Gaussian Noise\n",
    "The agent is currently using Gaussian Noise to encourage exploration of the action space. A noise factor of epsilon 0.3 is used consistently throughout the training. Annealing this to lower values based on performance once the agent comes close to convergence may result in higher maximum scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
