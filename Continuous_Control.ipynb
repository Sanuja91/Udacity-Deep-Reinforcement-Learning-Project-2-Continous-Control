{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Initialize the Environment \n",
    "Run the below section once to initialize the environment.. Don't Repeat!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Number of actions: 4\n",
      "States have length: 33\n",
      "States initialized: 20\n"
     ]
    }
   ],
   "source": [
    "from utilities import Seeds, initialize_env, get_device\n",
    "\n",
    "MULTI = True\n",
    "device = get_device()                           # gets gpu if available\n",
    "\n",
    "environment_params = {\n",
    "    'multiple_agents': MULTI,                   # runs 20 or 1 arm environment\n",
    "    'no_graphics': False,                       # runs no graphics windows version\n",
    "    'train_mode': True,                         # runs in train mode\n",
    "    'offline': True,                            # toggle on for udacity jupyter notebook\n",
    "    'agent_count': 20 if MULTI else 1,  \n",
    "    'device': device\n",
    "}\n",
    "\n",
    "env, env_info, states, state_size, action_size, brain_name, num_agents = initialize_env(environment_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configure the Agent\n",
    "Configures the respective 'knobs' on the agent to beat the environment.. The best configurations from my experiments have already been set.\n",
    "\n",
    "To load the complete version of the agent, please run as is. If you want test the agent from scratch, simply change the name in agent params and re run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from agent import DDPGAgent, D4PGAgent\n",
    "from train import train\n",
    "from memory import NStepReplayBuffer\n",
    "\n",
    "seedGenerator = Seeds('seeds')\n",
    "seedGenerator.next()\n",
    "\n",
    "experience_params = {\n",
    "    'seed': seedGenerator,                      # seed for the experience replay buffer\n",
    "    'buffer_size': 300000,                      # size of the replay buffer\n",
    "    'batch_size': 128,                         # batch size sampled from the replay buffer\n",
    "    'rollout': 5,                               # n step rollout length    \n",
    "    'agent_count': 20 if MULTI else 1,  \n",
    "    'gamma': 0.99,\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "experienceReplay = NStepReplayBuffer(experience_params)\n",
    "\n",
    "params = {\n",
    "    'episodes': 2000,                           # number of episodes\n",
    "    'maxlen': 100,                              # sliding window size of recent scores\n",
    "    'brain_name': brain_name,                   # the brain name of the unity environment\n",
    "    'achievement': 30.,                         # score at which the environment is considered solved\n",
    "    'environment': env,             \n",
    "    'pretrain': True,                           # whether pretraining with random actions should be done\n",
    "    'pretrain_length': 5000,                    # minimum experience required in replay buffer to start training \n",
    "    'random_fill': False,                       # basically repeat pretrain at specific times to encourage further exploration\n",
    "    'random_fill_every': 10000,             \n",
    "    'shape_rewards': True,                     # shapes 0 rewards into small negative reward\n",
    "    'negative_reward': -0.0001,\n",
    "    'log_dir': 'runs/',\n",
    "    'load_agent': True,\n",
    "    'agent_params': {\n",
    "        'name': 'D4PG New Shape Rewards with Noise Decay',\n",
    "        'experience_replay': experienceReplay,\n",
    "        'device': device,\n",
    "        'seed': seedGenerator,\n",
    "        'num_agents': num_agents,               # number of agents in the environment\n",
    "        'gamma': 0.99,                          # discount factor\n",
    "        'tau': 0.0001,                          # mixing rate soft-update of target parameters\n",
    "        'update_target_every': 350,             # update the target network every n-th step\n",
    "        'update_every': 1,                     # update the active network every n-th step\n",
    "        'actor_update_every_multiplier': 1,     # update actor every x timestep multiples of the crtic, critic needs time to adapt to new actor\n",
    "        'update_intensity': 1,                 # learns from the same experiences several times\n",
    "        'update_target_type': 'hard',           # should the update be soft at every time step or hard at every x timesteps\n",
    "        'add_noise': True,                      # add noise using 'noise_params'\n",
    "        'anneal_noise': True,  \n",
    "        'schedule_lr': False,                   # schedule learning rates \n",
    "        'lr_steps': 30,                         # step iterations to cycle lr using cosine\n",
    "        'lr_reset_every': 5000,                 # steps learning rate   \n",
    "        'lr_reduction_factor': 0.9,             # reduce lr on plateau reduction factor\n",
    "        'lr_patience_factor': 10,                # reduce lr after x (timesteps/episodes) not changing tracked item\n",
    "        'actor_params': {                       # actor parameters\n",
    "            'lr': 0.0005,                       # learning rate\n",
    "            'state_size': state_size,           # size of the state space\n",
    "            'action_size': action_size,         # size of the action space\n",
    "            'seed': seedGenerator,              # seed of the network architecture\n",
    "            'dropout': 0.05,\n",
    "        },\n",
    "        'critic_params': {                      # critic parameters\n",
    "            'lr': 0.001,                         # learning rate\n",
    "            'weight_decay': 3e-10,              # weight decay\n",
    "            'state_size': state_size,           # size of the state space\n",
    "            'action_size': action_size,         # size of the action space\n",
    "            'seed': seedGenerator,              # seed of the network architecture\n",
    "            'dropout': 0.05,\n",
    "            'action_layer': True,\n",
    "            'num_atoms': 75,\n",
    "            'v_min': 0.0, \n",
    "            'v_max': 0.5\n",
    "        },\n",
    "        'ou_noise_params': {                    # parameters for the Ornstein Uhlenbeck process\n",
    "            'mu': 0.,                           # mean\n",
    "            'theta': 0.15,                      # theta value for the ornstein-uhlenbeck process\n",
    "            'sigma': 0.2,                       # variance\n",
    "            'seed': seedGenerator,              # seed\n",
    "            'action_size': action_size\n",
    "        },\n",
    "        'ge_noise_params': {                    # parameters for the Gaussian Exploration process                   \n",
    "            'max_epsilon': 0.3,                 \n",
    "            'min_epsilon': 0.005,\n",
    "            'decay_epsilon': True,    \n",
    "            'patience_episodes': 2,         # episodes since the last best reward  \n",
    "            'decay_rate': 0.95                   \n",
    "        },\n",
    "        \n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
