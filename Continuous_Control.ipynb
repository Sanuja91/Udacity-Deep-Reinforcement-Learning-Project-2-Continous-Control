{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Initialize the Environment \n",
    "Run the below section once to initialize the environment.. Don't Repeat!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Number of actions: 4\n",
      "States have length: 33\n",
      "States initialized: 20\n"
     ]
    }
   ],
   "source": [
    "from utilities import Seeds, initialize_env, get_device\n",
    "\n",
    "MULTI = True\n",
    "device = get_device()                           # gets gpu if available\n",
    "\n",
    "environment_params = {\n",
    "    'multiple_agents': MULTI,                   # runs 20 or 1 arm environment\n",
    "    'no_graphics': False,                       # runs no graphics windows version\n",
    "    'train_mode': True,                         # runs in train mode\n",
    "    'offline': True,                            # toggle on for udacity jupyter notebook\n",
    "    'agent_count': 20 if MULTI else 1,  \n",
    "    'device': device\n",
    "}\n",
    "\n",
    "env, env_info, states, state_size, action_size, brain_name, num_agents = initialize_env(environment_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configure the Agent\n",
    "Configures the respective 'knobs' on the agent to beat the environment.. The best configurations from my experiments have already been set.\n",
    "\n",
    "To load the complete version of the agent, please run as is. If you want test the agent from scratch, simply change the name in agent params and re run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from agent import D4PGAgent\n",
    "from train import train\n",
    "from memory import NStepReplayBuffer\n",
    "from noise import OUNoise, GaussianExploration\n",
    "\n",
    "\n",
    "seedGenerator = Seeds('seeds')\n",
    "seedGenerator.next()\n",
    "\n",
    "experience_params = {\n",
    "    'seed': seedGenerator,                      # seed for the experience replay buffer\n",
    "    'buffer_size': 300000,                      # size of the replay buffer\n",
    "    'batch_size': 128,                          # batch size sampled from the replay buffer\n",
    "    'rollout_length': 5,                        # n step rollout length    \n",
    "    'agent_count': 20 if MULTI else 1,  \n",
    "    'gamma': 0.99,\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "experienceReplay = NStepReplayBuffer(experience_params)\n",
    "\n",
    "noise_params = {\n",
    "    'ou_noise_params': {                        # parameters for the Ornstein Uhlenbeck process\n",
    "        'mu': 0.,                               # mean\n",
    "        'theta': 0.15,                          # theta value for the ornstein-uhlenbeck process\n",
    "        'sigma': 0.2,                           # variance\n",
    "        'seed': seedGenerator,                  # seed\n",
    "        'action_size': action_size  \n",
    "    },  \n",
    "    'ge_noise_params': {                        # parameters for the Gaussian Exploration process                   \n",
    "        'max_epsilon': 0.3,                     \n",
    "        'min_epsilon': 0.005,   \n",
    "        'decay_epsilon': False,      \n",
    "        'patience_episodes': 2,                 # episodes since the last best reward  \n",
    "        'decay_rate': 0.95                   \n",
    "    }\n",
    "}\n",
    "\n",
    "noise = GaussianExploration(noise_params['ge_noise_params'])\n",
    "\n",
    "params = {\n",
    "    'episodes': 2000,                           # number of episodes\n",
    "    'maxlen': 100,                              # sliding window size of recent scores\n",
    "    'brain_name': brain_name,                   # the brain name of the unity environment\n",
    "    'achievement': 30.,                         # score at which the environment is considered solved\n",
    "    'environment': env,             \n",
    "    'pretrain': True,                           # whether pretraining with random actions should be done\n",
    "    'pretrain_length': 5000,                    # minimum experience required in replay buffer to start training \n",
    "    'random_fill': False,                       # basically repeat pretrain at specific times to encourage further exploration\n",
    "    'random_fill_every': 10000,             \n",
    "    'shape_rewards': True,                      # shapes 0 rewards into small negative rewards\n",
    "    'negative_reward': -0.0001,\n",
    "    'log_dir': 'runs/',\n",
    "    'load_agent': True,\n",
    "    'agent_params': {\n",
    "        'name': 'D4PG with Shape Rewards',\n",
    "        'd4pg': True,\n",
    "        'experience_replay': experienceReplay,\n",
    "        'device': device,\n",
    "        'seed': seedGenerator,\n",
    "        'num_agents': num_agents,               # number of agents in the environment\n",
    "        'gamma': 0.99,                          # discount factor\n",
    "        'tau': 0.0001,                          # mixing rate soft-update of target parameters\n",
    "        'update_target_every': 350,             # update the target network every n-th step\n",
    "        'update_every': 1,                      # update the active network every n-th step\n",
    "        'actor_update_every_multiplier': 1,     # update actor every x timestep multiples of the crtic, critic needs time to adapt to new actor\n",
    "        'update_intensity': 1,                  # learns from the same experiences several times\n",
    "        'update_target_type': 'hard',           # should the update be soft at every time step or hard at every x timesteps\n",
    "        'add_noise': True,                      # add noise using 'noise_params'\n",
    "        'anneal_noise': True,  \n",
    "        'schedule_lr': False,                   # schedule learning rates \n",
    "        'lr_steps': 30,                         # step iterations to cycle lr using cosine\n",
    "        'lr_reset_every': 5000,                 # steps learning rate   \n",
    "        'lr_reduction_factor': 0.9,             # reduce lr on plateau reduction factor\n",
    "        'lr_patience_factor': 10,               # reduce lr after x (timesteps/episodes) not changing tracked item\n",
    "        'actor_params': {                       # actor parameters\n",
    "            'lr': 0.0005,                       # learning rate\n",
    "            'state_size': state_size,           # size of the state space\n",
    "            'action_size': action_size,         # size of the action space\n",
    "            'seed': seedGenerator,              # seed of the network architecture\n",
    "        },\n",
    "        'critic_params': {                      # critic parameters\n",
    "            'lr': 0.001,                        # learning rate\n",
    "            'weight_decay': 3e-10,              # weight decay\n",
    "            'state_size': state_size,           # size of the state space\n",
    "            'action_size': action_size,         # size of the action space\n",
    "            'seed': seedGenerator,              # seed of the network architecture\n",
    "            'action_layer': True,\n",
    "            'num_atoms': 75,\n",
    "            'v_min': 0.0, \n",
    "            'v_max': 0.5\n",
    "        },\n",
    "        'noise': noise\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agent\n",
    "Get the agent to learn to beat the environment and output the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################ ACTOR ################\n",
      "\n",
      "Actor(\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=33, out_features=400, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc2): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=300, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc3): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=4, bias=True)\n",
      "    (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Tanh()\n",
      "  )\n",
      ")\n",
      "\n",
      "################ CRITIC ################\n",
      "\n",
      "D4PGCritic(\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=37, out_features=400, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc2): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=300, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc3): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=75, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n",
      "\n",
      "Cannot find D4PG with Shape Rewards checkpoint... Proceeding to create fresh neural network\n",
      "\n",
      "Episode 1\tAverage Score: 0.42 \t Min: 0.00 \t Max: 1.15 \t Time: 70.03\n",
      "Episode 2\tAverage Score: 0.72 \t Min: 0.00 \t Max: 2.07 \t Time: 91.07\n",
      "Episode 3\tAverage Score: 2.84 \t Min: 0.89 \t Max: 5.36 \t Time: 94.18\n",
      "Timestep 3006\tScore: 0.00\tmin: 0.00\tmax: 0.00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-69ffd0cc2d08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0magents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mD4PGAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'agent_params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_processes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_agents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'episode'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'D4PG'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Dropbox\\Coding\\AI\\Udacity\\Deep Reinforcement Learning\\deep-reinforcement-learning\\p2_continuous-control\\train.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(agents, params, num_processes)\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0madjusted_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0madjusted_rewards\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'negative_reward'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mactor_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madjusted_rewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpretrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpretrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mactor_loss\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcritic_loss\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Dropbox\\Coding\\AI\\Udacity\\Deep Reinforcement Learning\\deep-reinforcement-learning\\p2_continuous-control\\agent.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, states, actions, rewards, next_states, dones, pretrain)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpretrain\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Dropbox\\Coding\\AI\\Udacity\\Deep Reinforcement Learning\\deep-reinforcement-learning\\p2_continuous-control\\agent.py\u001b[0m in \u001b[0;36mlearn_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m                     \u001b[1;31m# Calculate the projected log probabilities from the target actor and critic networks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m                     \u001b[1;31m# Since back propogation is not required. Tensors are detach to increase speed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m                     \u001b[0mtarget_dist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m                     \u001b[1;31m# The critic loss is calculated using a weighted distribution instead of the mean to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Dropbox\\Coding\\AI\\Udacity\\Deep Reinforcement Learning\\deep-reinforcement-learning\\p2_continuous-control\\agent.py\u001b[0m in \u001b[0;36m_get_targets\u001b[1;34m(self, rewards, next_states)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;31m# Project the categorical distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[0mprojected_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mprojected_probs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Dropbox\\Coding\\AI\\Udacity\\Deep Reinforcement Learning\\deep-reinforcement-learning\\p2_continuous-control\\agent.py\u001b[0m in \u001b[0;36m_get_value_distribution\u001b[1;34m(self, rewards, probs)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m             \u001b[0mprojected_probs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_add_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower_bound\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_lower\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m             \u001b[0mprojected_probs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_add_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupper_bound\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_upper\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mprojected_probs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agents = D4PGAgent(params=params['agent_params']) \n",
    "\n",
    "scores = train(agents=agents, params=params, num_processes=num_agents)\n",
    "\n",
    "df = pd.DataFrame(data={'episode': np.arange(len(scores)), 'D4PG': scores})\n",
    "df.to_csv('results/D4PG.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
