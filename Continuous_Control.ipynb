{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Initialize the Environment \n",
    "Run the below section once to initialize the environment.. Don't Repeat!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Number of actions: 4\n",
      "States have length: 33\n",
      "States initialized: 20\n"
     ]
    }
   ],
   "source": [
    "from utilities import Seeds, initialize_env, get_device\n",
    "\n",
    "MULTI = True\n",
    "device = get_device()                           # gets gpu if available\n",
    "\n",
    "environment_params = {\n",
    "    'multiple_agents': MULTI,                   # runs 20 or 1 arm environment\n",
    "    'no_graphics': False,                       # runs no graphics windows version\n",
    "    'train_mode': True,                         # runs in train mode\n",
    "    'offline': True,                            # toggle on for udacity jupyter notebook\n",
    "    'agent_count': 20 if MULTI else 1,  \n",
    "    'device': device\n",
    "}\n",
    "\n",
    "env, env_info, states, state_size, action_size, brain_name, num_agents = initialize_env(environment_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configure the Agent\n",
    "Configures the respective 'knobs' on the agent to beat the environment.. The best configurations from my experiments have already been set.\n",
    "\n",
    "To load the complete version of the agent, please run as is. If you want test the agent from scratch, simply change the name in agent params and re run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from agent import D4PGAgent\n",
    "from train import train\n",
    "from memory import NStepReplayBuffer\n",
    "from noise import OUNoise, GaussianExploration\n",
    "\n",
    "\n",
    "seedGenerator = Seeds('seeds')\n",
    "seedGenerator.next()\n",
    "\n",
    "experience_params = {\n",
    "    'seed': seedGenerator,                      # seed for the experience replay buffer\n",
    "    'buffer_size': 300000,                      # size of the replay buffer\n",
    "    'batch_size': 128,                          # batch size sampled from the replay buffer\n",
    "    'rollout_length': 5,                        # n step rollout length    \n",
    "    'agent_count': 20 if MULTI else 1,  \n",
    "    'gamma': 0.99,\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "experienceReplay = NStepReplayBuffer(experience_params)\n",
    "\n",
    "noise_params = {\n",
    "    'ou_noise_params': {                        # parameters for the Ornstein Uhlenbeck process\n",
    "        'mu': 0.,                               # mean\n",
    "        'theta': 0.15,                          # theta value for the ornstein-uhlenbeck process\n",
    "        'sigma': 0.2,                           # variance\n",
    "        'seed': seedGenerator,                  # seed\n",
    "        'action_size': action_size  \n",
    "    },  \n",
    "    'ge_noise_params': {                        # parameters for the Gaussian Exploration process                   \n",
    "        'max_epsilon': 0.3,                     \n",
    "        'min_epsilon': 0.005,   \n",
    "        'decay_epsilon': False,      \n",
    "        'patience_episodes': 2,                 # episodes since the last best reward  \n",
    "        'decay_rate': 0.95                   \n",
    "    }\n",
    "}\n",
    "\n",
    "noise = GaussianExploration(noise_params['ge_noise_params'])\n",
    "\n",
    "params = {\n",
    "    'episodes': 2000,                           # number of episodes\n",
    "    'maxlen': 100,                              # sliding window size of recent scores\n",
    "    'brain_name': brain_name,                   # the brain name of the unity environment\n",
    "    'achievement': 30.,                         # score at which the environment is considered solved\n",
    "    'environment': env,             \n",
    "    'pretrain': True,                           # whether pretraining with random actions should be done\n",
    "    'pretrain_length': 5000,                    # minimum experience required in replay buffer to start training \n",
    "    'random_fill': False,                       # basically repeat pretrain at specific times to encourage further exploration\n",
    "    'random_fill_every': 10000,             \n",
    "    'shape_rewards': True,                      # shapes 0 rewards into small negative rewards\n",
    "    'negative_reward': -0.0001,\n",
    "    'log_dir': 'runs/',\n",
    "    'load_agent': True,\n",
    "    'agent_params': {\n",
    "        'name': 'D4PG with Shape Rewards',\n",
    "        'd4pg': True,\n",
    "        'experience_replay': experienceReplay,\n",
    "        'device': device,\n",
    "        'seed': seedGenerator,\n",
    "        'num_agents': num_agents,               # number of agents in the environment\n",
    "        'gamma': 0.99,                          # discount factor\n",
    "        'tau': 0.0001,                          # mixing rate soft-update of target parameters\n",
    "        'update_target_every': 350,             # update the target network every n-th step\n",
    "        'update_every': 1,                      # update the active network every n-th step\n",
    "        'actor_update_every_multiplier': 1,     # update actor every x timestep multiples of the crtic, critic needs time to adapt to new actor\n",
    "        'update_intensity': 1,                  # learns from the same experiences several times\n",
    "        'update_target_type': 'hard',           # should the update be soft at every time step or hard at every x timesteps\n",
    "        'add_noise': True,                      # add noise using 'noise_params'\n",
    "        'anneal_noise': True,  \n",
    "        'schedule_lr': False,                   # schedule learning rates \n",
    "        'lr_steps': 30,                         # step iterations to cycle lr using cosine\n",
    "        'lr_reset_every': 5000,                 # steps learning rate   \n",
    "        'lr_reduction_factor': 0.9,             # reduce lr on plateau reduction factor\n",
    "        'lr_patience_factor': 10,               # reduce lr after x (timesteps/episodes) not changing tracked item\n",
    "        'actor_params': {                       # actor parameters\n",
    "            'lr': 0.0005,                       # learning rate\n",
    "            'state_size': state_size,           # size of the state space\n",
    "            'action_size': action_size,         # size of the action space\n",
    "            'seed': seedGenerator,              # seed of the network architecture\n",
    "        },\n",
    "        'critic_params': {                      # critic parameters\n",
    "            'lr': 0.001,                        # learning rate\n",
    "            'weight_decay': 3e-10,              # weight decay\n",
    "            'state_size': state_size,           # size of the state space\n",
    "            'action_size': action_size,         # size of the action space\n",
    "            'seed': seedGenerator,              # seed of the network architecture\n",
    "            'action_layer': True,\n",
    "            'num_atoms': 75,\n",
    "            'v_min': 0.0, \n",
    "            'v_max': 0.5\n",
    "        },\n",
    "        'noise': noise\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agent\n",
    "Get the agent to learn to beat the environment and output the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################ ACTOR ################\n",
      "\n",
      "Actor(\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=33, out_features=400, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc2): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=300, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc3): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=4, bias=True)\n",
      "    (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Tanh()\n",
      "  )\n",
      ")\n",
      "\n",
      "################ CRITIC ################\n",
      "\n",
      "D4PGCritic(\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=37, out_features=400, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc2): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=300, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc3): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=75, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Actor:\n\tMissing key(s) in state_dict: \"fc3.0.weight\", \"fc3.0.bias\", \"fc3.1.weight\", \"fc3.1.bias\", \"fc3.1.running_mean\", \"fc3.1.running_var\". \n\tUnexpected key(s) in state_dict: \"fc5.0.weight\", \"fc5.0.bias\", \"fc5.1.weight\", \"fc5.1.bias\", \"fc5.1.running_mean\", \"fc5.1.running_var\", \"fc5.1.num_batches_tracked\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-69ffd0cc2d08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0magents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mD4PGAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'agent_params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_processes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_agents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'episode'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'D4PG'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Dropbox\\Coding\\AI\\Udacity\\Deep Reinforcement Learning\\deep-reinforcement-learning\\p2_continuous-control\\train.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(agents, params, num_processes)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mepisode_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'load_agent'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mepisode_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimesteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi_episode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Dropbox\\Coding\\AI\\Udacity\\Deep Reinforcement Learning\\deep-reinforcement-learning\\p2_continuous-control\\agent.py\u001b[0m in \u001b[0;36mload_agent\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    137\u001b[0m             \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilePath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_active\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'actor_state_dict'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'actor_state_dict'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_active\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'critic_state_dict'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanuja-xps\\appdata\\local\\conda\\conda\\envs\\ml-agents\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m    767\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m--> 769\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m    770\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_named_members\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_members_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Actor:\n\tMissing key(s) in state_dict: \"fc3.0.weight\", \"fc3.0.bias\", \"fc3.1.weight\", \"fc3.1.bias\", \"fc3.1.running_mean\", \"fc3.1.running_var\". \n\tUnexpected key(s) in state_dict: \"fc5.0.weight\", \"fc5.0.bias\", \"fc5.1.weight\", \"fc5.1.bias\", \"fc5.1.running_mean\", \"fc5.1.running_var\", \"fc5.1.num_batches_tracked\". "
     ]
    }
   ],
   "source": [
    "agents = D4PGAgent(params=params['agent_params']) \n",
    "\n",
    "scores = train(agents=agents, params=params, num_processes=num_agents)\n",
    "\n",
    "df = pd.DataFrame(data={'episode': np.arange(len(scores)), 'D4PG': scores})\n",
    "df.to_csv('results/D4PG.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
